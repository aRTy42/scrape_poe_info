#! python3
"""
# scrape_poe_maps.py - scrapes poe maps from http://pathofexile.gamepedia.com/Map
# (modified from scrape_poe_uniques.py)
"""

import requests, bs4, re, datetime, time, json, os
from bs4 import NavigableString
from multiprocessing.dummy import Pool as ThreadPool

SCRIPTDIR = os.path.dirname(os.path.abspath(__file__))

base_url = 'http://pathofexile.gamepedia.com'
main_url = base_url + '/User:ARTyficial/MapData'

rx_search = re.compile(r'\+*\(([\d\.]+)\s[a-z]+\s([\d\.]+)[)]|(\+*[\d\.]+\%)|([\d\.]+-[\d\.]+)|(\([\d\.]+-[\d\.]+)\s\w+\s([\d\.]+-[\d\.]+\))|(-?\+?[\d\.]+)')
vendor_regex = re.compile('yields? one|produces? one', re.IGNORECASE)
maptype_regex = re.compile('Map type', re.IGNORECASE)


def write_file_headers():
	"""
	info headers for MapList.txt

	:return: list
	"""
	data = []
	d = datetime.datetime.now()
	now_time = d.strftime('%Y-%m-%d at %H:%M:%S')
	data.append('; Data from ' + main_url)
	data.append('; Comments can be made with ";", blank lines will be ignored.')
	data.append(';')
	data.append('; This file was auto-generated by scrape_poe_maps.py on {}'.format(now_time) + '\n')
	data.append('mapList := Object()')
	data.append('mapList["Unknown Map"] := "Map not recognised or not supported"\n')
	data.append('uniqueMapList := Object()')
	data.append('uniqueMapList["Unknown Map"] := "Map not recognised or not supported"\n')

	return data


def get_main_page(url):
	"""
	Gets the main wiki page for Maps and parses out the links for each map
	and returns a list of the urls
	:param url: main page url
	:return: list of urls
	"""
	map_urls = []
	print('Getting main page...')
	page = requests.get(url)
	page.raise_for_status()
	soup = bs4.BeautifulSoup(page.text, 'html.parser')
	map_table = soup.find_all('table', class_='wikitable')
	mapcount = 1
	for row in map_table[0].find_all('tr'):
		tds = row.find_all('td')
		if len(tds) == 0:	# exclude header row
			continue
		map_info = {}
		map_info['count'] = mapcount
		mapcount += 1
		map_info['tier'] = tds[0].text.strip()
		map_info['level'] = tds[1].text.strip()
		map_info['name'] = tds[2].text.strip()
		map_info['url'] = base_url + '/' + map_info['name'].replace(' ', '_')
		map_info['unique'] = tds[3].find('img')['alt'].strip()
		map_info['shaped'] = 'no'
		map_info['tileset'] = tds[4].text.strip()
		map_info['atlasposition'] = tds[5].text.strip()
		map_info['producedby'] = tds[6].text.strip()
		map_info['upgradesto'] = tds[7].text.strip()
		map_info['connectedto'] = tds[8].text.strip()
		map_info['boss'] = tds[9].text.strip()
		map_info['base'] = None
		if map_info['unique'] == 'yes':
			map_info['base'] = map_info['connectedto']
			map_info['connectedto'] = ''
		else:
			map_info['base'] = map_info['name']

		map_urls.append(map_info)
	
		# insert shaped map to re-use some of the data
		if int(map_info['tier']) < 11 and map_info['unique'] == 'no':
			map_info_sh = {}
			map_info_sh['count'] = mapcount
			mapcount += 1
			map_info_sh['tier'] = str(int(map_info['tier'])+5)
			map_info_sh['level'] = str(int(map_info_sh['tier'])+67)
			map_info_sh['name'] = 'Shaped ' + map_info['name']
			map_info_sh['url'] = base_url + '/' + map_info_sh['name'].replace(' ', '_')
			map_info_sh['unique'] = 'no'
			map_info_sh['shaped'] = 'yes'
			map_info_sh['tileset'] = map_info['tileset']
			map_info_sh['atlasposition'] = map_info['atlasposition']
			map_info_sh['producedby'] = None
			map_info_sh['upgradesto'] = None	# will be filled later with page query information
			map_info_sh['connectedto'] = None
			map_info_sh['boss'] = map_info['boss']
			map_info_sh['base'] = map_info['base']
			map_urls.append(map_info_sh)

	return map_urls


def parse_map_data(map_info):
	"""
	fetches the page for a map
	:param links:
	:return:
	"""
	page = requests.get(map_info['url'])
	page.raise_for_status()
	soup = bs4.BeautifulSoup(page.text, 'html.parser')
	return build_data(soup, map_info)


def find_divcards(div):
	for h2 in div.find_all('h2'):
		if h2.find('span', id='Divination_cards'):
			return h2
	return None
	
def find_vendor_recipe(div):
	try:
		for yields in div.find_all(text=vendor_regex):
			return yields.next_sibling.findNext('a').findNext('a').text
	except:
		return None
	return None
	

def find_setting(div):
	try:
		for maptype in div.find_all(text=maptype_regex):
			return maptype.parent.next_sibling.replace(':', '').strip()
	except:
		return None
	return None


def build_data(data, mapinfo):
	"""
	parse map data from the page
	:param data: BS4 ResultSet
	:return: list
	"""
	map_data = dict(mapinfo)
	print('Getting data for {}'.format(map_data['name']))

	div = data.find('div', id='mw-content-text')  # main div containing our data

	# find the h2 element containing "Divination Cards" and list items following it
	map_data['divcards'] = []
	div_h2 = find_divcards(div)
	if div_h2 is not None:
		divlist = div_h2.find_next_siblings('ul')
		if divlist is not None:
			divcards = divlist[0].find_all('li')
			for divcard in divcards:
				map_data['divcards'].append(divcard.find('a').text)

	# find the text string "yield(s) one/produce(s) one" to determine the vendor recipe for the map
	if map_data['shaped'] == 'yes':		# other recipes are already filled via wiki's User:ARTyficial/MapData table
		map_data['upgradesto'] = find_vendor_recipe(div)

	# find the map setting (indoors/outdoors)
	map_data['setting'] = find_setting(div)

	#if map_data['unique'] == 'no' and map_data['tier'] != 'N/A' and int(map_data['tier']) != 15 and map_data['vendor'] is None:
	#	print('No vendor found for ' + map_data['name'])
	#if map_data['setting'] is None:
	#	print('No setting found for ' + map_data['name'])

	return map_data


def convert_data_to_AHK_readable_format(all_data):
	"""
	This function takes the raw web page data, and converts it into lines that are readable by the
	Poe_item_info AHK script.
	:return:
	"""

	# Read the descriptions of various maps
	with open(SCRIPTDIR + '\\MapDescriptions.json', 'r') as f:
		map_descriptions = json.load(f)

	new_data = []
	vendor_map = {}
	unique_map = {}
	matchList = []
	matchList_shaped = []
	for mymap in all_data:  # scan once to determine the lower tier map(s) that vendor up to each map
		#if mymap['vendor'] is not None and mymap['unique'] == 'no':
		#	if not mymap['vendor'] in vendor_map:
		#		vendor_map[mymap['vendor']] = []
		#	vendor_map[mymap['vendor']].append(mymap['name'])
		if mymap['unique'] == 'no':
			if mymap['shaped'] == 'no':
				matchList.append(mymap['name'])
			if mymap['shaped'] == 'yes':
				matchList_shaped.append(mymap['name'])
		if mymap['unique'] == 'yes' and mymap['base'] is not None:
			unique_map[mymap['base']] = mymap['name']

	# shaped maps before other maps so that the script tries to match "Shaped Xyz Map" before "Xyz Map"
	# lists sorted by descending name length to avoid mismatching ("Spider Lair Map" before "Lair Map" etc.)
	matchList.sort(key=len, reverse=True)
	matchList_shaped.sort(key=len, reverse=True)
	
	new_data.append('mapMatchList := ["' + '","'.join(matchList) + '"]\n')
	new_data.append('shapedMapMatchList := ["' + '","'.join(matchList_shaped) + '"]\n')	

	for mymap in all_data:
		if mymap['base'] is None:
			print('ERROR: Could not determine base map type for map: ' + mymap['name'])
			continue
		if mymap['unique'] == 'yes':
			new_data.append(';' + mymap['name'])
			line = 'uniqueMapList["' + mymap['base'] + '"] := "'
		else:
			line = 'mapList["' + mymap['name'] + '"] := "'
		
		line += 'Tier: ' + mymap['tier'] + ', Level: ' + mymap['level'] + ', Atlas position: ' + mymap['atlasposition']
		line += '`nTileset: ' + mymap['tileset']
		if mymap['setting'] is not None:
			line += ' (' + mymap['setting'] + ')'

		# Add on vendor recipes and connected maps
		if mymap['unique'] == 'no':
			info_lines = []
			#if mymap['base'] in vendor_map:
			#	for vend in vendor_map[mymap['base']]:
			#		vendor_lines.append('`n- Produced by three ' + vend + 's')
			if mymap['producedby']:
				info_lines.append('`nProduced by: ' + mymap['producedby'])
			else:
				info_lines.append('`nProduced by: none')
			if mymap['upgradesto']:
				info_lines.append('`nUpgrades to: ' + mymap['upgradesto'])
			else:
				info_lines.append('`nUpgrades to: ?')
			if mymap['connectedto']:
				info_lines.append('`nAlso connected to: ' + mymap['connectedto'])
			if len(info_lines) > 0:
				line += '`n'
				for il in info_lines:
					line += il
		
		# Add line when map is shaped
		if mymap['shaped'] == 'yes':
			line += '`n`nInfos from ' + mymap['base'] + ':'

		# Add on unique version if one exists
		if mymap['base'] in unique_map and mymap['unique'] == 'no':
			line += '`n`nUnique version of map: ' + unique_map[mymap['base']]

		# Add on divination cards
		if len(mymap['divcards']) > 0:
			line += '`n`nDivination cards:'
			for dc in mymap['divcards']:
				line += '`n- ' + dc
				
		# Here we insert the prewritten text descriptions for the maps that have them
		desc_key = 'maps'
		if mymap['unique'] == 'yes':
			desc_key = 'uniqueMaps'
		if mymap['base'] in map_descriptions[desc_key]:
			line += '`n`n' + map_descriptions[desc_key][mymap['base']]

		line += '"\n'
		new_data.append(line)

	return new_data


def write(new_data):
	file = open(SCRIPTDIR + '\\MapList.txt', 'a+b')  # opens file for writing
	for row in new_data:
		file.write(row.encode('cp1252'))
		file.write(b'\n')
	file.close()


def main():
	map_list = get_main_page(main_url)
	open(SCRIPTDIR + '\\MapList.txt', 'w').close()  # create file (or overwrite it if it exists)
	write(write_file_headers())
	pool = ThreadPool(4)
	data = pool.map(parse_map_data, map_list)
	data.sort(key=lambda m: m['count'])
	x = convert_data_to_AHK_readable_format(data)
	write(x)
	pool.close()
	pool.join()


startTime = datetime.datetime.now()
main()
print('Program execution time: ',(datetime.datetime.now() - startTime))
